{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web scraping Marca and El Pais 2.0\n",
    "\n",
    "In a <a href=\"https://github.com/edu9as/web-scraping/blob/master/Scraping-Marca-and-El-Pais.ipynb\">notebook</a> I wrote some weeks ago, I showed some complex code to obtain some headlines or complete news using Python. And when I say complex, I say really complex :(\n",
    "\n",
    "These past weeks I have been reading about web scraping. Specifically, this <a href=\"https://www.amazon.com/Web-Scraping-Python-Collecting-Modern/dp/1491985577\">book</a> from Ryan Mitchell has been very useful for me. Now, I can perform the same task as in the previous notebook with much simpler code and with better results. \n",
    "\n",
    "## Step 1: Load some libraries\n",
    "\n",
    "In this case, we need the same two libraries as in the previous notebook, plus **json**. This last package will be useful to decipher information in some webpages that is present in JSON format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create a function to extract the news from the webpages\n",
    "\n",
    "Here, I am defining a function that is allows the scraping of all news in these two webpages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_new(url):\n",
    "    try:\n",
    "        print(\"#\"*80)\n",
    "        page = requests.get(url).text\n",
    "        soup = BeautifulSoup(page, \"html.parser\")\n",
    "        for article in soup.findAll(\"article\"):\n",
    "            for p in article.findAll({\"p\", \"h3\", \"h2\", \"li\"} if not \"marca\" in url \n",
    "                                     else {\"p\", \"h3\"}):\n",
    "                if p.get(\"class\") == [\"nombre\"]:\n",
    "                    print(p.text.strip(), \"says:\")\n",
    "                elif p.get(\"class\") and p.name ==\"p\" or p.text.strip() == \"\":\n",
    "                    continue\n",
    "                else:\n",
    "                    print(p.text.strip(), \"\\n\")\n",
    "                    if p.text.strip() == \"Inicia sesión para seguir leyendo\":\n",
    "                        print(\"I'm sorry I can't do it now, please show me this new:)\\n\")\n",
    "                        print(json.loads(soup.findAll(\"script\")[1].string)[\"articleBody\"])\n",
    "                        break\n",
    "        print(\"#\"*80)\n",
    "    \n",
    "    except:\n",
    "        print(\"wowwwww problem with:\\n\", url)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- I want exactly 80 asterisks right before and after each new.\n",
    "- All news in these two online newsletters are found within **article** tags.\n",
    "- In the case of <a href=\"https://www.marca.com\">Marca</a>, the text of the news can be found between **p** and **h3** tags.\n",
    "- In the case of <a href=\"https://www.elpais.com\">El Pais</a>, the news bodies can be found between **p**, **h3**, **h2** and **li** tags.\n",
    "- In Marca, at the end of each new we can find some comments the users of this newsletter have made. I considered this to be useful. Username is found between **p** tags with attribute **class** being **nombre**, and the comment itself comes right after the name of the user within another pair of **p** tags.\n",
    "- For some news in El Pais, it is mandatory to login before reading the news (*Inicia sesión para seguir leyendo* between **p** tags). I am not able to do that using Python, but I noticed that the body of the new can be found at the beginning of the source code, between **script** tags and in JSON format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Let's read Marca news\n",
    "\n",
    "In this step, I am requesting the main <a href=\"https://www.marca.com\">marca.com</a> webpage. All headlines are found within **main** tags, and more specifically within **a** tags with **itemprop** attribute being **url**. For each **a** tag with the corresponding headline, the news' text can be accessed thanks to their **href** attribute. If the user of my code enters any character after my question, the news' text is printed to the console.\n",
    "\n",
    "Note: some news are just a video. As this cannot be shown in the console, those news are skipped with a warning message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titulo = \"# MARCA.com #\"\n",
    "sep_lat = \" \"*int((80-len(titulo))/2)\n",
    "\n",
    "print(sep_lat + \"#\"*len(titulo) + sep_lat + \"\\n\" + \n",
    "      sep_lat + titulo + sep_lat + \"\\n\" +\n",
    "      sep_lat + \"#\"*len(titulo) + sep_lat)\n",
    "\n",
    "url = \"https://www.marca.com\"\n",
    "page = requests.get(url).text\n",
    "soup = BeautifulSoup(page, \"html.parser\")\n",
    "\n",
    "\n",
    "for titular in soup.main.findAll(\"a\", itemprop = \"url\"):\n",
    "    print(\"\\n-\",titular.text.strip())\n",
    "    if titular.get(\"href\")[8:11] == \"vid\":\n",
    "        print(\"### This new is a video, not available here ###\")\n",
    "    elif input(\"Do you want to read more about it? \"):\n",
    "        scrape_new(titular.get(\"href\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Let's read El Pais news\n",
    "\n",
    "I have done the same for <a href=\"https://www.elpais.com\">elpais.com</a>.\n",
    "\n",
    "As a particularity, some El Pais news' links are in the form of \"/...\" and some others are like \"https://...\". It is important to distinguish between these two types of links in order to access each new."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titulo = \"# ElPais.com #\"\n",
    "sep_lat = \" \"*int((80-len(titulo))/2)\n",
    "\n",
    "print(sep_lat + \"#\"*len(titulo) + sep_lat + \"\\n\" + \n",
    "      sep_lat + titulo + sep_lat + \"\\n\" +\n",
    "      sep_lat + \"#\"*len(titulo) + sep_lat)\n",
    "\n",
    "\n",
    "url = \"https://elpais.com\"\n",
    "page = requests.get(url).text\n",
    "soup = BeautifulSoup(page, \"html.parser\")\n",
    "\n",
    "\n",
    "for titular in soup.find(\"div\", class_=\"flex_grid background_white\").findAll(\"a\"):\n",
    "    if titular.get(\"class\") == None:\n",
    "        if titular.text == \"Suscripciones\":\n",
    "            break\n",
    "        print(titular.text)\n",
    "        if input(\"More information? \"):\n",
    "            if titular.get(\"href\")[0:4] != \"http\":\n",
    "                scrape_new(url + titular.get(\"href\"))\n",
    "            else:\n",
    "                scrape_new(titular.get(\"href\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Have fun!\n",
    "\n",
    "This is the end of this notebook. I hope you have enjoyed it.\n",
    "\n",
    "I think this code can be used not only to print the news to the console, but also to analyze some tendencies in the news media with natural language processing. This knowledge can be applied to other goals, and surely I will work in this line during the following weeks.\n",
    "\n",
    "See you!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
