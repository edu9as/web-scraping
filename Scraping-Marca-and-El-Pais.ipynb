{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do you want to escape ads when you are visiting a news website?\n",
    "\n",
    "Maybe your favourite Spanish-news diary, which might be www.elpais.com. Or your favourite Spanish sport-news newspaper, www.marca.com. The copious amount of ads you have to face when you visit these webpages some days gives you a hard time. \n",
    "\n",
    "What if you could read all the news you wanted, only with a click, very quickly, no ads? Well, continue reading because this post is for you. Welcome!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web scraping\n",
    "\n",
    "Web scraping is a technique we use for extracting data from a website. (Almost) whichever website you imagine. You can read the source code of the website, notice where the data you need is found, and you can easily obtain what you want.\n",
    "\n",
    "In this post, I am using web scraping for obtaining news (title, subtitle, theme, body, whatever) from www.elpais.com and www.marca.com. The language I am using is Python, a very powerfull programming language. Thanks to some functionalities brought by **requests** and **BeautifulSoup** packages, we can easily parse the web in search of some good (or bad) news."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Marca\n",
    "\n",
    "Quoting Wikipedia, \"Marca is a Spanish national daily sport newspaper owned by Unidad Editorial. The newspaper focuses primarily on football, in particular the day-to-day activities of Real Madrid, FC Barcelona and Atlético Madrid\" (source: https://en.wikipedia.org/wiki/Marca_(newspaper)). Reading this newspaper, you can stay up to date on sports information, not only about these three football teams but also about many different topics such as basketball, tennis or Formula-1.\n",
    "\n",
    "Years ago, I read this newspaper daily on the web (www.marca.com). However, over the years, they have included tons of advertisements in their webpage, and now it is very slow to read news in this website. And I am pretty impatient, so I have written a piece of code to get all trending news in this webpage and read only those you are interested in, without any advertisement.\n",
    "\n",
    "First, I have defined a function to look for all the news webpages that we want to read and print them to the console in a clean manner, called **print_marca_new**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_marca_new(new_):\n",
    "    url = new_.get(\"href\")\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.text, \"html.parser\")\n",
    "    \n",
    "    print(\"Title:\")\n",
    "    try:\n",
    "        title = BeautifulSoup(str(soup.find_all(\"h1\")[0]), \"html.parser\").text.strip()\n",
    "        print(\"   \" + title)\n",
    "    except:\n",
    "        print(\"   Not available\")\n",
    "    try:\n",
    "        subtitles = BeautifulSoup(str(soup.find_all(\"h2\")[0]), \"html.parser\")\n",
    "        theme = BeautifulSoup(str(subtitles.find_all(\"span\")[0]), \"html.parser\").text.strip()\n",
    "        subtitle = BeautifulSoup(str(subtitles.find_all(\"span\")[1]), \"html.parser\").text.strip()\n",
    "        print(\"Theme:\\n   \", theme)\n",
    "        print(\"Summary:\\n   \", subtitle)\n",
    "    except:\n",
    "        subtitle = BeautifulSoup(str(soup.find_all(\"p\")[0]), \"html.parser\").text.strip()\n",
    "        print(\"Summary:\\n   \", subtitle)\n",
    "    \n",
    "    print(\"\\n\")\n",
    "        \n",
    "    if input(\"Continue reading?(y): \") == \"y\":\n",
    "        try:\n",
    "            body = BeautifulSoup(str(soup.find_all(\"div\", class_=\"row content cols-30-70\")[0]), \"html.parser\")\n",
    "            paragraphs = body.find_all([\"p\", \"h3\"])[0:-1]\n",
    "            print(\"Content:\\n\")\n",
    "            for paragraph in paragraphs:\n",
    "                print(paragraph.text, \"\\n\")\n",
    "        except:\n",
    "            print(\"Not available new.\")\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Knowing how the main webpage of Marca is structured, we can get all the news from this website very easily:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.marca.com/\"\n",
    "page = requests.get(url)\n",
    "soup = BeautifulSoup(page.text, \"html.parser\")\n",
    "\n",
    "tit = soup.find_all(\"a\", itemprop = \"url\")\n",
    "subt = soup.find_all(\"h2\", class_=\"flex-article__heading\")\n",
    "\n",
    "print(\"##################\\n\" +\n",
    "      \"# marca.com news #\\n\" +\n",
    "      \"##################\\n\")\n",
    "for _ in range(len(tit)):\n",
    "    a = BeautifulSoup(str(soup.find_all(\"a\", itemprop = \"url\")[_]), \"html.parser\").text.strip()\n",
    "    if input(\"-\" + a + \"     Read this new?(y): \") == \"y\":\n",
    "        print_marca_new(tit[_])\n",
    "        \n",
    "     \n",
    "print(\"NOWWWWW\")\n",
    "for _ in range(len(subt)):\n",
    "    h2 = BeautifulSoup(str(soup.find_all(\"h2\", class_ = \"flex-article__heading\")[_]), \"html.parser\")\n",
    "    a = h2.find_all(\"a\")[0]\n",
    "    t = BeautifulSoup(str(a), \"html.parser\").text.strip()\n",
    "    if input(\"-\" + t + \"     Read new?(y): \") == \"y\":\n",
    "        print_marca_new(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## El País\n",
    "\n",
    "Another time, I'm quoting Wikipedia to briefly explain what this newspaper is: \"El País is a Spanish-language daily newspaper in Spain. El País is based in the capital city of Madrid and it is owned by the Spanish media conglomerate PRISA. According to the Office of Justification of Dissemination, it is the second most circulated daily newspaper in Spain as of December 2017.\" (source: https://en.wikipedia.org/wiki/El_Pa%C3%ADs)\n",
    "\n",
    "This is a newspaper that I read sometimes, and I have web-scraped it just for fun. Also, this code is useful to avoid adverts when you want to read the body of a new.\n",
    "\n",
    "The function I have defined to extract and print all news in a clean manner is this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_elpais_new(new_):\n",
    "    url = new_.get(\"href\")\n",
    "    \n",
    "    if url[0]==\"/\":\n",
    "        url = \"https://elpais.com\"+url\n",
    "        page = requests.get(url)\n",
    "        if not page:\n",
    "            next\n",
    "            \n",
    "        soup = BeautifulSoup(page.text, \"html.parser\")        \n",
    "                             \n",
    "        article = BeautifulSoup(str(soup.find_all(\"article\")), \"html.parser\")\n",
    "        titles = BeautifulSoup(str(article.find_all(\"div\", id = \"article_header\")), \"html.parser\")\n",
    "        theme = titles.find_all(\"span\")\n",
    "        try:\n",
    "            theme = theme[0].text\n",
    "        except:\n",
    "            theme = \"\"\n",
    "            \n",
    "        title = titles.find_all(\"h1\")\n",
    "        try:\n",
    "            title = title[0].text\n",
    "        except:\n",
    "            print(\"Noticia no disponible\")\n",
    "            \n",
    "        subtitle = titles.find_all(\"h2\")\n",
    "        try:\n",
    "            subtitle = subtitle[0].text\n",
    "        except:\n",
    "            subtitle = \"\"\n",
    "            \n",
    "        print(\"\\n   Tema: {}\\n   Titular: {}\\n   Subtitular: {}\\n   Noticia:\\n\".format(theme, title, subtitle))\n",
    "        \n",
    "        paragraphs = BeautifulSoup(str(article.find_all(\"div\", class_=\"a_b article_body | color_gray_dark\")), \"html.parser\")\n",
    "        paragraphs = paragraphs.find_all(\"p\")\n",
    "        for paragraph in paragraphs:\n",
    "            if paragraph.text[0:14] != \"Este navegador\":\n",
    "                print(paragraph.text, \"\\n\")\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And with this last block of code, you can access the text of all news in elpais.com:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.elpais.com/\"\n",
    "page = requests.get(url)\n",
    "soup = BeautifulSoup(page.text, \"html.parser\")\n",
    "                     \n",
    "news = BeautifulSoup(str(soup.find_all(\"div\", class_ = \"section_b | col desktop_12 tablet_8 mobile_4\")), \"html.parser\")\n",
    "tit1 = news.find_all(\"a\", class_ = [\"\", \"related_story_headline\"])\n",
    "\n",
    "sections = {\"section_b\": \"portada\", \"section_c\": \"destacados\",\n",
    "          \"thematic_section\": \"sección temática\"}\n",
    "\n",
    "\n",
    "for section in [\"section_b\", \"section_c\", \"thematic_section\"]:\n",
    "    name = sections[section].capitalize()\n",
    "    print(\"\\n\\n\" + \"#\"*(len(name) + 4) + \"\\n\" +\n",
    "          \"# %s #\\n\" % name +\n",
    "          \"#\"*(len(name) + 4) + \"\\n\")\n",
    "    block = BeautifulSoup(str(soup.find_all(\"div\", class_ = section+\" | col desktop_12 tablet_8 mobile_4\")), \"html.parser\")\n",
    "    if section != \"thematic_section\":\n",
    "        for _class_ in [\"\", \"related_story_headline\"]:\n",
    "            titles = block.find_all(\"a\", class_ = _class_)\n",
    "            for _ in range(len(titles)):\n",
    "                a = input(\"-\" + BeautifulSoup(str(titles[_]), \"html.parser\").text + \"    ¿Leer noticia?(y): \")\n",
    "                if a == \"y\":\n",
    "                    print_elpais_new(titles[_])\n",
    "    elif section == \"thematic_section\":\n",
    "        for topic in [\"internacional\", \"espana\", \"el-pais-economia\",\n",
    "                          \"sociedad\", \"cultura\", \"ciencia-y-tecnologia\",\n",
    "                          \"deportes\", \"television\", \"estilo-y-vida\", \"gente\",\n",
    "                          \"motor\"]:\n",
    "            print(\"\\n##\", apartados.upper(), \"##\\n\")\n",
    "            category = BeautifulSoup(str(block.find_all(\"div\", id= topic)), \"html.parser\")\n",
    "            for _class_ in [\"\", \"related_story_headline\"]:\n",
    "                titles = category.find_all(\"a\", class_ = _class_)\n",
    "                for _ in range(len(titles)):\n",
    "                    a = input(\"-\" + BeautifulSoup(str(titles[_]), \"html.parser\").text + \"   ¿Leer noticia entera?(y): \")\n",
    "\n",
    "                    if a==\"y\":\n",
    "                        print_elpais_new(titles[_])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I hope you have enjoyed the content of this notebook. Web scraping is a whole new world and it brings a lot of possibilities for data science, so I recommend you to learn some HTML, JavaScript or similar languages if you are interested in this topic, because it has almost no limits.\n",
    "\n",
    "Have fun"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
